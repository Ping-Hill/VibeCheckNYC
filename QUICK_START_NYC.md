# Quick Start: NYC Version

## TL;DR - Convert to NYC in 4 Steps

### Step 1: Update Config (1 line change)

```bash
# Edit config.py line 194
# Change from:
SEARCH_QUERIES = DC_SEARCH_QUERIES

# To:
SEARCH_QUERIES = NYC_SEARCH_QUERIES
```

### Step 2: Set API Key

```bash
export SERPAPI_API_KEY="your_serpapi_key_here"
```

### Step 3: Run Data Pipeline

```bash
# Scrape NYC restaurants (~2-3 weeks due to API quotas)
python scripts/serpapi_full_scraper.py

# Generate embeddings (~2-4 hours with GPU)
python src/vibecheck/embeddings/generator.py

# Load database (~10 minutes)
python scripts/load_sql.py

# Generate visualizations (~15 minutes)
python src/vibecheck/analysis/vibe_mapper.py
```

### Step 4: Launch App

```bash
python app/app.py
# Visit http://localhost:8080
```

---

## What Changed in the Cleanup

### ‚úÖ Deleted (12 files)
- 5 outdated scraper scripts
- 4 legacy processing scripts
- 1 Gradio app (unused)
- 1 test placeholder
- 1 duplicate directory

### ‚úÖ Created (3 files)
- `config.py` - Centralized configuration
- `ARCHITECTURE.md` - System documentation
- `CLEANUP_SUMMARY.md` - Detailed cleanup notes

### ‚úÖ Updated (1 file)
- `scripts/serpapi_full_scraper.py` - Now uses `config.py`

---

## Current Structure

```
VibeCheck/
‚îú‚îÄ‚îÄ config.py                    # ‚öôÔ∏è  START HERE for NYC config
‚îú‚îÄ‚îÄ app/app.py                   # üåê PRIMARY web app
‚îú‚îÄ‚îÄ api/main.py                  # üîå REST API
‚îú‚îÄ‚îÄ scripts/serpapi_full_scraper.py  # üì• Data collection
‚îî‚îÄ‚îÄ src/vibecheck/               # üß† ML pipeline
```

---

## Code Reusability

**95% of code works for any city** with zero changes:
- ‚úÖ All ML models (CLIP, BERT, FAISS)
- ‚úÖ All web apps (Flask, FastAPI)
- ‚úÖ All database code
- ‚úÖ All embedding/clustering code

**Only 1 line needs updating:**
- üîß `config.py` line 194: Switch DC ‚Üí NYC queries

---

## NYC Coverage (Pre-Configured)

Already configured in `config.py`:

**60+ neighborhoods:**
- Manhattan: 20 (SoHo, Greenwich Village, Chelsea, ...)
- Brooklyn: 15 (Williamsburg, DUMBO, Park Slope, ...)
- Queens: 8 (Astoria, Long Island City, Flushing, ...)
- Bronx: 4 (Mott Haven, Riverdale, Fordham, ...)
- Staten Island: 2 (St. George, Stapleton)

**Expected data:**
- ~6,000+ restaurants (vs 2,700 DC)
- ~18,000 SerpAPI calls needed
- ~500MB total data size

---

## Quick Reference Commands

```bash
# View config summary
python -c "from config import *; print(get_config_summary())"

# Test scraper works
python -c "from scripts.serpapi_full_scraper import *"

# Run full DC pipeline (test before NYC)
python app/app.py  # Should work with existing DC data

# Check SerpAPI quota
curl "https://serpapi.com/account?api_key=YOUR_KEY"
```

---

## Files You Can Delete (Optional)

If you want even more cleanup:

```bash
# Old documentation (outdated now)
rm DOCKER_QUICKSTART.md DOCKER_SETUP.md DATA_SETUP.md
# (We have better docs now in ARCHITECTURE.md)

# Old deployment guides
rm DOCKER_MIGRATION_SUMMARY.md QUICKSTART_HF.md
# (Deployment info is in README.md)
```

---

## Need Help?

- **System design:** Read `ARCHITECTURE.md`
- **What changed:** Read `CLEANUP_SUMMARY.md`
- **Getting started:** Read `README.md`
- **Configuration:** Read `config.py` inline comments

---

**Ready to build NYC VibeCheck! üóΩ**
